{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "The target variable will be the car's price. The predictor's variables will include make, model, year, mileage, vehicle condition and location. The goal of the assignment is to build a predictive model that properly quantifies the impact the aforementioned features on the car's price. This insight can provide actionable recommendations to the used car dealership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "First step I would take is open the file in Excel to get familiar with the data prior to doing any notebook work with Pandas or Numpy. This preparation generally leads to either dropping columns that don't make sense or knowing the types of data just spot checking. In this case VIN will always be unique, so it can be dropped. In addition the size column is rarely populated so that can be dropped.\n",
    "\n",
    "After doing the spot checking/getting comfortable with the data in Excel, my first step in the notebook is to load dataset into a pandas dataframe and then get an overview of the data through a call to the head and tail function as found during my spot checking - the first several rows are misleading from a missing value perspective. Using the describe, info and isnull methods to further evaluate what is in the dataset and determine what level of data preparation will be required. Lastly, I would use the unique method to determine the unique values of each column to determine their cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Data Investigation\n",
    "The following code shows the investigation performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas and read in the csv into a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n",
      "           id                  region  price  year manufacturer model  \\\n",
      "0  7222695916                prescott   6000   NaN          NaN   NaN   \n",
      "1  7218891961            fayetteville  11900   NaN          NaN   NaN   \n",
      "2  7221797935            florida keys  21000   NaN          NaN   NaN   \n",
      "3  7222270760  worcester / central MA   1500   NaN          NaN   NaN   \n",
      "4  7210384030              greensboro   4900   NaN          NaN   NaN   \n",
      "\n",
      "  condition cylinders fuel  odometer title_status transmission  VIN drive  \\\n",
      "0       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "1       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "2       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "3       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "4       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "\n",
      "  size type paint_color state  \n",
      "0  NaN  NaN         NaN    az  \n",
      "1  NaN  NaN         NaN    ar  \n",
      "2  NaN  NaN         NaN    fl  \n",
      "3  NaN  NaN         NaN    ma  \n",
      "4  NaN  NaN         NaN    nc  \n",
      "Tail\n",
      "                id   region  price    year manufacturer  \\\n",
      "426875  7301591192  wyoming  23590  2019.0       nissan   \n",
      "426876  7301591187  wyoming  30590  2020.0        volvo   \n",
      "426877  7301591147  wyoming  34990  2020.0     cadillac   \n",
      "426878  7301591140  wyoming  28990  2018.0        lexus   \n",
      "426879  7301591129  wyoming  30590  2019.0          bmw   \n",
      "\n",
      "                           model condition    cylinders    fuel  odometer  \\\n",
      "426875         maxima s sedan 4d      good  6 cylinders     gas   32226.0   \n",
      "426876  s60 t5 momentum sedan 4d      good          NaN     gas   12029.0   \n",
      "426877          xt4 sport suv 4d      good          NaN  diesel    4174.0   \n",
      "426878           es 350 sedan 4d      good  6 cylinders     gas   30112.0   \n",
      "426879  4 series 430i gran coupe      good          NaN     gas   22716.0   \n",
      "\n",
      "       title_status transmission                VIN drive size       type  \\\n",
      "426875        clean        other  1N4AA6AV6KC367801   fwd  NaN      sedan   \n",
      "426876        clean        other  7JR102FKXLG042696   fwd  NaN      sedan   \n",
      "426877        clean        other  1GYFZFR46LF088296   NaN  NaN  hatchback   \n",
      "426878        clean        other  58ABK1GG4JU103853   fwd  NaN      sedan   \n",
      "426879        clean        other  WBA4J1C58KBM14708   rwd  NaN      coupe   \n",
      "\n",
      "       paint_color state  \n",
      "426875         NaN    wy  \n",
      "426876         red    wy  \n",
      "426877       white    wy  \n",
      "426878      silver    wy  \n",
      "426879         NaN    wy  \n"
     ]
    }
   ],
   "source": [
    "# Look at the columns from the dataframe\n",
    "print('Head')\n",
    "print(df.head())\n",
    "# Shows a lot of NaN values\n",
    "\n",
    "print('Tail')\n",
    "print(df.tail())\n",
    "\n",
    "# However, tail does not - indicating a brash decision on just the head is not logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id         price           year      odometer\n",
      "count  4.268800e+05  4.268800e+05  425675.000000  4.224800e+05\n",
      "mean   7.311487e+09  7.519903e+04    2011.235191  9.804333e+04\n",
      "std    4.473170e+06  1.218228e+07       9.452120  2.138815e+05\n",
      "min    7.207408e+09  0.000000e+00    1900.000000  0.000000e+00\n",
      "25%    7.308143e+09  5.900000e+03    2008.000000  3.770400e+04\n",
      "50%    7.312621e+09  1.395000e+04    2013.000000  8.554800e+04\n",
      "75%    7.315254e+09  2.648575e+04    2017.000000  1.335425e+05\n",
      "max    7.317101e+09  3.736929e+09    2022.000000  1.000000e+07\n"
     ]
    }
   ],
   "source": [
    "# Inspect the variation \n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426880 entries, 0 to 426879\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            426880 non-null  int64  \n",
      " 1   region        426880 non-null  object \n",
      " 2   price         426880 non-null  int64  \n",
      " 3   year          425675 non-null  float64\n",
      " 4   manufacturer  409234 non-null  object \n",
      " 5   model         421603 non-null  object \n",
      " 6   condition     252776 non-null  object \n",
      " 7   cylinders     249202 non-null  object \n",
      " 8   fuel          423867 non-null  object \n",
      " 9   odometer      422480 non-null  float64\n",
      " 10  title_status  418638 non-null  object \n",
      " 11  transmission  424324 non-null  object \n",
      " 12  VIN           265838 non-null  object \n",
      " 13  drive         296313 non-null  object \n",
      " 14  size          120519 non-null  object \n",
      " 15  type          334022 non-null  object \n",
      " 16  paint_color   296677 non-null  object \n",
      " 17  state         426880 non-null  object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 58.6+ MB\n",
      "None\n",
      "id                   0\n",
      "region               0\n",
      "price                0\n",
      "year              1205\n",
      "manufacturer     17646\n",
      "model             5277\n",
      "condition       174104\n",
      "cylinders       177678\n",
      "fuel              3013\n",
      "odometer          4400\n",
      "title_status      8242\n",
      "transmission      2556\n",
      "VIN             161042\n",
      "drive           130567\n",
      "size            306361\n",
      "type             92858\n",
      "paint_color     130203\n",
      "state                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify Missing Values and get the data types of the columns\n",
    "\n",
    "print(df.info())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "In looking at the null counts - the following columns are going to be my focus:\n",
    "region, year, manufacturer, model, fuel, odometer, title_status, transmission and state\n",
    "\n",
    "Other columns such as condition and type would be interesting to look at as well, but given larger % of nulls I would be hessistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: 404 unique values\n",
      "manufacturer: 42 unique values\n",
      "model: 29649 unique values\n",
      "condition: 6 unique values\n",
      "cylinders: 8 unique values\n",
      "fuel: 5 unique values\n",
      "title_status: 6 unique values\n",
      "transmission: 3 unique values\n",
      "VIN: 118246 unique values\n",
      "drive: 3 unique values\n",
      "size: 4 unique values\n",
      "type: 13 unique values\n",
      "paint_color: 12 unique values\n",
      "state: 51 unique values\n"
     ]
    }
   ],
   "source": [
    "# Establish the unique values for each column\n",
    "\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    print(f'{col}: {df[col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Na\n",
    "df = df.dropna(subset=['price', 'year', 'odometer']) \n",
    "df = df.fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: 404 unique values\n",
      "manufacturer: 43 unique values\n",
      "model: 29219 unique values\n",
      "condition: 7 unique values\n",
      "cylinders: 9 unique values\n",
      "fuel: 6 unique values\n",
      "title_status: 7 unique values\n",
      "transmission: 4 unique values\n",
      "VIN: 116164 unique values\n",
      "drive: 4 unique values\n",
      "size: 5 unique values\n",
      "type: 14 unique values\n",
      "paint_color: 13 unique values\n",
      "state: 51 unique values\n"
     ]
    }
   ],
   "source": [
    "# Re-inspect the dataframe after some clean up\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    print(f'{col}: {df[col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables\n",
    "categorical_features = ['region', 'manufacturer', 'model', 'condition', 'cylinders', 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state'] \n",
    "df = pd.get_dummies(df, columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train and test split\n",
    "\n",
    "# After inspecting values VIN number can't help the business case, dropping that column as well as price and id\n",
    "X = df.drop(['price', 'id', 'VIN'], axis=1) \n",
    "y = df['price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 74.8 GiB for an array with shape (29775, 337075) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\AppData\\Local\\Temp\\ipykernel_108760\\1122312211.py\", line 7, in fit_model\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 578, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1263, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 521, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2152, in __array__\n    values = self._values\n             ^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 1127, in _values\n    return ensure_wrapped_if_datetimelike(self.values)\n                                          ^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 12664, in values\n    return self._mgr.as_array()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1694, in as_array\n    arr = self._interleave(dtype=dtype, na_value=na_value)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jcape\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1727, in _interleave\n    result = np.empty(self.shape, dtype=dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 74.8 GiB for an array with shape (29775, 337075) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     11\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     LinearRegression(),\n\u001b[0;32m     13\u001b[0m     RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m ]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Parallelize the model fitting\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m trained_models \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(delayed(fit_model)(model, X_train, y_train) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Example of accessing the trained models\u001b[39;00m\n\u001b[0;32m     20\u001b[0m linear_model, random_forest_model, xgb_model \u001b[38;5;241m=\u001b[39m trained_models\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 74.8 GiB for an array with shape (29775, 337075) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define function to fit and return a model\n",
    "def fit_model(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# List of models to train\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "]\n",
    "\n",
    "# Parallelize the model fitting\n",
    "trained_models = Parallel(n_jobs=-1)(delayed(fit_model)(model, X_train, y_train) for model in models)\n",
    "\n",
    "# Example of accessing the trained models\n",
    "linear_model, random_forest_model, xgb_model = trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
